{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Setup Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import soundfile as sf\n",
    "import python_speech_features\n",
    "from python_speech_features import mfcc\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import pickle\n",
    "\n",
    "print(\"Initial Setup Complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Environment Bias\n",
      "\n",
      "envbias shape (60, 4043)\n",
      "envbias shape (4043,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing Environment Bias\\n\")\n",
    "envbias = np.empty([60,4043])                    \n",
    "# reading all the back.wav files, coverting to mfcc format, adding labels and storing in an array\n",
    "for j in range(0,60):\n",
    "    b = \"data_augmented/env/env\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.resize(4043,)\n",
    "    envbias[j]=data\n",
    "\n",
    "print(\"envbias shape {}\".format(envbias.shape))\n",
    "envbias = np.array(np.sum(envbias,axis=0))/60.0\n",
    "print(\"envbias shape {}\".format(envbias.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data Set\n",
      "\n",
      "y0l shape (2000, 8091)\n",
      "y1l shape (2000, 8091)\n",
      "y2l shape (2000, 8091)\n",
      "y3l shape (2000, 8091)\n",
      "y4l shape (2000, 8091)\n",
      "\n",
      "Shuffling\n",
      "\n",
      "\n",
      "y0l shape (2000, 8091)\n",
      "y1l shape (2000, 8091)\n",
      "y2l shape (2000, 8091)\n",
      "y3l shape (2000, 8091)\n",
      "y4l shape (2000, 8091)\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing Data Set\\n\")\n",
    "y0 = np.empty([2000,8086])                    \n",
    "# reading all the back.wav files, coverting to mfcc format, adding labels and storing in an array\n",
    "for j in range(0,2000):\n",
    "    b = \"full_dataset/back\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    #data = np.array(data) - np.resize(envbias[:min(len(envbias),len(np.array(data)))],len(np.array(data)))\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.reshape(4043,)\n",
    "    data=np.array(list(data)+list(envbias))\n",
    "    y0[j]=data\n",
    "y = np.empty([2000,5])\n",
    "for i in range (0,2000):                      \n",
    "    # manually assigning labels\n",
    "    y[i][0]=1.0\n",
    "    y[i][1]=0.0\n",
    "    y[i][2]=0.0\n",
    "    y[i][3]=0.0\n",
    "    y[i][4]=0.0 \n",
    "y0l = np.append(y0,y,axis=1)\n",
    "print(\"y0l shape {}\".format(y0l.shape))\n",
    "\n",
    "y1 = np.empty([2000,8086])\n",
    "\n",
    "for j in range(0,2000):                       \n",
    "    # reading all the forward.wav files, coverting to mfcc format, adding labels and storing in an array\n",
    "    b = \"full_dataset/forward\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    #data = np.array(data) - np.resize(envbias[:min(len(envbias),len(np.array(data)))],len(np.array(data)))\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.reshape(4043,)\n",
    "    data=np.array(list(data)+list(envbias))\n",
    "    y1[j]=data\n",
    "y = np.empty([2000,5])\n",
    "for i in range (0,2000):                      # manually assigning labels\n",
    "    y[i][0]=0.0\n",
    "    y[i][1]=1.0\n",
    "    y[i][2]=0.0\n",
    "    y[i][3]=0.0\n",
    "    y[i][4]=0.0\n",
    "y1l = np.append(y1,y,axis=1)\n",
    "print(\"y1l shape {}\".format(y1l.shape))  \n",
    "\n",
    "y2 = np.empty([2000,8086])\n",
    "\n",
    "for j in range(0,2000):                        \n",
    "    # reading all the left.wav files, coverting to mfcc format, adding labels and storing in an array  \n",
    "    b = \"full_dataset/left\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    #data = np.array(data) - np.resize(envbias[:min(len(envbias),len(np.array(data)))],len(np.array(data)))\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.reshape(4043,)\n",
    "    data=np.array(list(data)+list(envbias))\n",
    "    y2[j]=data\n",
    "y = np.empty([2000,5])\n",
    "for i in range (0,2000):                        # manually assigning labels\n",
    "    y[i][0]=0.0\n",
    "    y[i][1]=0.0\n",
    "    y[i][2]=1.0\n",
    "    y[i][3]=0.0\n",
    "    y[i][4]=0.0\n",
    "y2l = np.append(y2,y,axis=1)\n",
    "print(\"y2l shape {}\".format(y2l.shape))\n",
    "\n",
    "y3 = np.empty([2000,8086])                      \n",
    "# reading all the right.wav files, coverting to mfcc format, adding labels and storing in an array\n",
    "for j in range(0,2000):\n",
    "    b = \"full_dataset/right\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    #data = np.array(data) - np.resize(envbias[:min(len(envbias),len(np.array(data)))],len(np.array(data)))\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.reshape(4043,)\n",
    "    data=np.array(list(data)+list(envbias))\n",
    "    y3[j]=data\n",
    "y = np.empty([2000,5])\n",
    "for i in range (0,2000):                        # manually assigning labels\n",
    "    y[i][0]=0.0\n",
    "    y[i][1]=0.0\n",
    "    y[i][2]=0.0\n",
    "    y[i][3]=1.0\n",
    "    y[i][4]=0.0\n",
    "y3l = np.append(y3,y,axis=1)\n",
    "print(\"y3l shape {}\".format(y3l.shape))  \n",
    "\n",
    "y4 = np.empty([2000,8086])  \n",
    "# reading all the stop.wav files, coverting to mfcc format, adding labels and storing in an array   \n",
    "for j in range(0,2000):\n",
    "    b = \"full_dataset/stop\"+str(j)+\".wav\"\n",
    "    #print b\n",
    "    data, samplerate = sf.read(b)\n",
    "    #data = np.array(data) - np.resize(envbias[:min(len(envbias),len(np.array(data)))],len(np.array(data)))\n",
    "    data1 = mfcc(data,samplerate)\n",
    "    data = data1.reshape(4043,)\n",
    "    data=np.array(list(data)+list(envbias))\n",
    "    y4[j]=data\n",
    "y = np.empty([2000,5])\n",
    "for i in range (0,2000):                         \n",
    "    # manually assigning labels\n",
    "    y[i][0]=0.0\n",
    "    y[i][1]=0.0\n",
    "    y[i][2]=0.0\n",
    "    y[i][3]=0.0\n",
    "    y[i][4]=1.0\n",
    "y4l = np.append(y4,y,axis=1)\n",
    "print(\"y4l shape {}\".format(y4l.shape))\n",
    "\n",
    "print(\"\\nShuffling\\n\")\n",
    "np.random.shuffle(y0l)\n",
    "np.random.shuffle(y1l)\n",
    "np.random.shuffle(y2l)\n",
    "np.random.shuffle(y3l)\n",
    "np.random.shuffle(y4l)\n",
    "\n",
    "print()\n",
    "print(\"y0l shape {}\".format(y0l.shape))\n",
    "print(\"y1l shape {}\".format(y1l.shape))\n",
    "print(\"y2l shape {}\".format(y2l.shape))\n",
    "print(\"y3l shape {}\".format(y3l.shape))\n",
    "print(\"y4l shape {}\".format(y4l.shape))\n",
    "\n",
    "print(\"\\nDone\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Test/Train Set\n",
      "\n",
      "trains shape (8750, 8091)\n",
      "tests shape (1250, 8091)\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting Test/Train Set\\n\")\n",
    "\n",
    "\n",
    "\n",
    "trains = np.empty([8750,8091])                   # using the first 1750 elements of each word in the train set \n",
    "k=0\n",
    "for j in range(0,1750):\n",
    "    trains[j]=y0l[k]\n",
    "    k=k+1 \n",
    "k=0\n",
    "for j in range(1750,3500):\n",
    "    trains[j]=y1l[k]\n",
    "    k=k+1 \n",
    "k=0\n",
    "for j in range(3500,5250):\n",
    "    trains[j]=y2l[k]\n",
    "    k=k+1 \n",
    "k=0\n",
    "for j in range(5250,7000):\n",
    "    trains[j]=y3l[k]\n",
    "    k=k+1 \n",
    "k=0\n",
    "for j in range(7000,8750):\n",
    "    trains[j]=y4l[k]\n",
    "    k=k+1 \n",
    "print(\"trains shape {}\".format(trains.shape))\n",
    "np.random.shuffle(trains)\n",
    "\n",
    "tests = np.empty([1250,8091])                      # using the last 250 elements of each array in the test set\n",
    "k = 1750\n",
    "for j in range(0,250):\n",
    "    tests[j]=y0l[k]\n",
    "    k=k+1\n",
    "k = 1750    \n",
    "for j in range(250,500):\n",
    "    tests[j]=y1l[k]\n",
    "    k=k+1\n",
    "k = 1750 \n",
    "for j in range(500,750):\n",
    "    tests[j]=y2l[k]\n",
    "    k=k+1\n",
    "k = 1750 \n",
    "for j in range(750,1000):\n",
    "    tests[j]=y3l[k]\n",
    "    k=k+1 \n",
    "k = 1750\n",
    "for j in range(1000,1250):\n",
    "    tests[j]=y4l[k]\n",
    "    k=k+1 \n",
    "print(\"tests shape {}\".format(tests.shape))\n",
    "np.random.shuffle(tests)\n",
    "\n",
    "print(\"\\nDone\\n\")\n",
    "\n",
    "#for i in range(8750):\n",
    "#    print(trains[i,-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating Features and Labels\n",
      "\n",
      "trainX shape (8750, 8086)\n",
      "trainY shape (8750,)\n",
      "testX shape (1250, 8086)\n",
      "testY shape (1250,)\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Separating Features and Labels\\n\")\n",
    "\n",
    "trainX = np.empty([8750,8086])                         # spliting of train set into features and labels          \n",
    "trainY_v = np.empty([8750,5])\n",
    "trainY = np.empty(8750)\n",
    "for i in range(0,8750):\n",
    "    trainX[i]=trains[i][:8086]\n",
    "for i in range(0,8750):\n",
    "    trainY_v[i]=trains[i][-5:]\n",
    "for i in range(8750):\n",
    "    trainY[i] = np.argmax(trainY_v[i])\n",
    "print(\"trainX shape {}\".format(trainX.shape))\n",
    "print(\"trainY shape {}\".format(trainY.shape))\n",
    "\n",
    "testX = np.empty([1250,8086])                           # spliting of test set into features and labels  \n",
    "testY_v = np.empty([1250,5])\n",
    "testY = np.empty(1250)\n",
    "for i in range(0,1250):\n",
    "    testX[i]=tests[i][:8086]\n",
    "for i in range(0,1250):\n",
    "    testY_v[i]=tests[i][-5:]\n",
    "for i in range(1250):\n",
    "    testY[i] = np.argmax(testY_v[i])\n",
    "print(\"testX shape {}\".format(testX.shape))\n",
    "print(\"testY shape {}\".format(testY.shape))\n",
    "\n",
    "#Y now contains the labels as 0, 1, 2, 3, 4 respectively\n",
    "\n",
    "trainX=np.nan_to_num(trainX)\n",
    "testX=np.nan_to_num(testX)  \n",
    "trainY=np.nan_to_num(trainY)\n",
    "testY=np.nan_to_num(testY)    \n",
    "\n",
    "    \n",
    "print(\"\\nDone\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Trying Linear Discriminant Analysis (SVD)\\n\")\n",
    "\n",
    "\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "#clf1 = LDA(solver='svd')\n",
    "#clf1.fit(trainX, trainY)\n",
    "#predY1 = clf1.predict(testX)\n",
    "#acc1 = sklearn.metrics.accuracy_score(testY, predY1)\n",
    "#print(\"Accuracy = \",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Saving Trained LDA Model\")\n",
    "#file_name = '1_lda.sav'\n",
    "#pickle.dump(clf1,open(file_name,'wb'))\n",
    "#print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Random Forest Classifier\n",
      "\n",
      "Trying 300 estimators\n",
      "Accuracy =  0.9968\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying Random Forest Classifier\\n\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print(\"Trying 300 estimators\")\n",
    "clf2 = RandomForestClassifier(n_estimators=300,random_state=0)\n",
    "clf2.fit(trainX, trainY)\n",
    "predY2 = clf2.predict(testX)\n",
    "acc2 = sklearn.metrics.accuracy_score(testY, predY2)\n",
    "print(\"Accuracy = \",acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF300 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF300 Model\")\n",
    "file_name = '2_rf300.sav'\n",
    "pickle.dump(clf2,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 150 estimators\n",
      "Accuracy =  0.9952\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying 150 estimators\")\n",
    "clf3 = RandomForestClassifier(n_estimators=150,random_state=0,n_jobs=10)\n",
    "clf3.fit(trainX, trainY)\n",
    "predY3 = clf3.predict(testX)\n",
    "acc3 = sklearn.metrics.accuracy_score(testY, predY3)\n",
    "print(\"Accuracy = \",acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF150 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF150 Model\")\n",
    "file_name = '3_rf150.sav'\n",
    "pickle.dump(clf3,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 40 estimators\n",
      "Accuracy =  0.9928\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying 40 estimators\")\n",
    "clf4 = RandomForestClassifier(n_estimators=40,random_state=0)\n",
    "clf4.fit(trainX, trainY)\n",
    "predY4 = clf4.predict(testX)\n",
    "acc4 = sklearn.metrics.accuracy_score(testY, predY4)\n",
    "print(\"Accuracy = \",acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF40 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF40 Model\")\n",
    "file_name = '4_rf40.sav'\n",
    "pickle.dump(clf4,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Trying Linear Discriminant Analysis (LSQR)\\n\")\n",
    "\n",
    "\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "#clf7 = LDA(solver='lsqr')\n",
    "#clf7.fit(trainX, trainY)\n",
    "#predY7 = clf7.predict(testX)\n",
    "#acc7 = sklearn.metrics.accuracy_score(testY, predY7)\n",
    "#print(\"Accuracy = \",acc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Saving Trained LDA Model\")\n",
    "#file_name = '5_lda.sav'\n",
    "#pickle.dump(clf7,open(file_name,'wb'))\n",
    "#print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying MultiLayer Perceptron\n",
      "\n",
      "1 hidden Layer, Size =50\n",
      "\n",
      "Iteration 1, loss = 1.94741326\n",
      "Iteration 2, loss = 0.11342738\n",
      "Iteration 3, loss = 0.04622542\n",
      "Iteration 4, loss = 0.02322197\n",
      "Iteration 5, loss = 0.01151963\n",
      "Iteration 6, loss = 0.00703015\n",
      "Iteration 7, loss = 0.00522041\n",
      "Iteration 8, loss = 0.00409017\n",
      "Iteration 9, loss = 0.00334521\n",
      "Iteration 10, loss = 0.00278241\n",
      "Iteration 11, loss = 0.00235796\n",
      "Iteration 12, loss = 0.00203890\n",
      "Iteration 13, loss = 0.00179695\n",
      "Iteration 14, loss = 0.00157316\n",
      "Iteration 15, loss = 0.00141208\n",
      "Iteration 16, loss = 0.00127253\n",
      "Iteration 17, loss = 0.00115074\n",
      "Iteration 18, loss = 0.00104432\n",
      "Iteration 19, loss = 0.00095262\n",
      "Iteration 20, loss = 0.00087768\n",
      "Iteration 21, loss = 0.00081023\n",
      "Iteration 22, loss = 0.00074497\n",
      "Iteration 23, loss = 0.00069021\n",
      "Iteration 24, loss = 0.00064896\n",
      "Iteration 25, loss = 0.00060464\n",
      "Iteration 26, loss = 0.00056297\n",
      "Iteration 27, loss = 0.00052760\n",
      "Iteration 28, loss = 0.00049420\n",
      "Iteration 29, loss = 0.00046505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "acuracy= 0.988\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying MultiLayer Perceptron\\n\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "import sklearn.metrics as metric\n",
    "import numpy as np\n",
    "\n",
    "print(\"1 hidden Layer, Size =50\\n\")\n",
    "mlp11 = MLPClassifier(solver='adam', hidden_layer_sizes=(50,), activation='relu',verbose='False') # set the method\n",
    "mlp11.fit(trainX, trainY)                    # training\n",
    "predY_mlp11=mlp11.predict(testX)                      # prediction\n",
    "#print(y_pred)                                      # show the output\n",
    "acc_mlp11=sklearn.metrics.accuracy_score(testY, predY_mlp11)\n",
    "print('\\nacuracy=',acc_mlp11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained MLP50 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained MLP50 Model\")\n",
    "file_name = '6_mlp50.sav'\n",
    "pickle.dump(mlp11,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying MultiLayer Perceptron\n",
      "\n",
      "2 hidden Layer, Size =1000,500\n",
      "\n",
      "Iteration 1, loss = 2.99564247\n",
      "Iteration 2, loss = 0.05727911\n",
      "Iteration 3, loss = 0.02189438\n",
      "Iteration 4, loss = 0.00757660\n",
      "Iteration 5, loss = 0.00455079\n",
      "Iteration 6, loss = 0.00311178\n",
      "Iteration 7, loss = 0.00238115\n",
      "Iteration 8, loss = 0.00193129\n",
      "Iteration 9, loss = 0.00162164\n",
      "Iteration 10, loss = 0.00140035\n",
      "Iteration 11, loss = 0.00123507\n",
      "Iteration 12, loss = 0.00110294\n",
      "Iteration 13, loss = 0.00100540\n",
      "Iteration 14, loss = 0.00092727\n",
      "Iteration 15, loss = 0.00086197\n",
      "Iteration 16, loss = 0.00080814\n",
      "Iteration 17, loss = 0.00076808\n",
      "Iteration 18, loss = 0.00072723\n",
      "Iteration 19, loss = 0.00069412\n",
      "Iteration 20, loss = 0.00066598\n",
      "Iteration 21, loss = 0.00064339\n",
      "Iteration 22, loss = 0.00062242\n",
      "Iteration 23, loss = 0.00060459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "acuracy= 0.9968\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying MultiLayer Perceptron\\n\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "import sklearn.metrics as metric\n",
    "import numpy as np\n",
    "\n",
    "print(\"2 hidden Layer, Size =1000,500\\n\")\n",
    "mlp12 = MLPClassifier(solver='adam', hidden_layer_sizes=(1000,500), activation='relu',verbose='False') # set the method\n",
    "mlp12.fit(trainX, trainY)                    # training\n",
    "predY_mlp12=mlp12.predict(testX)                      # prediction\n",
    "#print(y_pred)                                      # show the output\n",
    "acc_mlp12=sklearn.metrics.accuracy_score(testY, predY_mlp12)\n",
    "print('\\nacuracy=',acc_mlp12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained MLP1000500 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained MLP1000500 Model\")\n",
    "file_name = '7_mlp1000500.sav'\n",
    "pickle.dump(mlp12,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 350 RF estimators\n",
      "Accuracy =  0.9976\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying 350 RF estimators\")\n",
    "clf14 = RandomForestClassifier(n_estimators=350,random_state=0,n_jobs=10)\n",
    "clf14.fit(trainX, trainY)\n",
    "predY14 = clf14.predict(testX)\n",
    "acc14 = sklearn.metrics.accuracy_score(testY, predY14)\n",
    "print(\"Accuracy = \",acc14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF350 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF350 Model\")\n",
    "file_name = '8_rf350.sav'\n",
    "pickle.dump(clf14,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying MultiLayer Perceptron\n",
      "\n",
      "1 hidden Layer, Size =1000\n",
      "\n",
      "Iteration 1, loss = 3.97283193\n",
      "Iteration 2, loss = 0.07174093\n",
      "Iteration 3, loss = 0.01827465\n",
      "Iteration 4, loss = 0.00650492\n",
      "Iteration 5, loss = 0.00261545\n",
      "Iteration 6, loss = 0.00168208\n",
      "Iteration 7, loss = 0.00127682\n",
      "Iteration 8, loss = 0.00110345\n",
      "Iteration 9, loss = 0.00096751\n",
      "Iteration 10, loss = 0.00087277\n",
      "Iteration 11, loss = 0.00080544\n",
      "Iteration 12, loss = 0.00074231\n",
      "Iteration 13, loss = 0.00069145\n",
      "Iteration 14, loss = 0.00065101\n",
      "Iteration 15, loss = 0.00061582\n",
      "Iteration 16, loss = 0.00058788\n",
      "Iteration 17, loss = 0.00055986\n",
      "Iteration 18, loss = 0.00053583\n",
      "Iteration 19, loss = 0.00051631\n",
      "Iteration 20, loss = 0.00049701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "acuracy= 0.9912\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying MultiLayer Perceptron\\n\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "import sklearn.metrics as metric\n",
    "import numpy as np\n",
    "\n",
    "print(\"1 hidden Layer, Size =1000\\n\")\n",
    "mlp15 = MLPClassifier(solver='adam', hidden_layer_sizes=(1000,), activation='relu',verbose='False') # set the method\n",
    "mlp15.fit(trainX, trainY)                    # training\n",
    "predY_mlp15=mlp15.predict(testX)                      # prediction\n",
    "#print(y_pred)                                      # show the output\n",
    "acc_mlp15=sklearn.metrics.accuracy_score(testY, predY_mlp15)\n",
    "print('\\nacuracy=',acc_mlp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained MLP1000 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained MLP1000 Model\")\n",
    "file_name = '9_mlp1000.sav'\n",
    "pickle.dump(mlp15,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying MultiLayer Perceptron\n",
      "\n",
      "1 hidden Layer, Size =100\n",
      "\n",
      "Iteration 1, loss = 1.20815676\n",
      "Iteration 2, loss = 0.46890131\n",
      "Iteration 3, loss = 0.22203599\n",
      "Iteration 4, loss = 0.11764116\n",
      "Iteration 5, loss = 0.06926535\n",
      "Iteration 6, loss = 0.05125082\n",
      "Iteration 7, loss = 0.03946086\n",
      "Iteration 8, loss = 0.02865860\n",
      "Iteration 9, loss = 0.02178435\n",
      "Iteration 10, loss = 0.01785903\n",
      "Iteration 11, loss = 0.01402455\n",
      "Iteration 12, loss = 0.01099666\n",
      "Iteration 13, loss = 0.00889492\n",
      "Iteration 14, loss = 0.00735528\n",
      "Iteration 15, loss = 0.00603204\n",
      "Iteration 16, loss = 0.00513516\n",
      "Iteration 17, loss = 0.00449353\n",
      "Iteration 18, loss = 0.00402779\n",
      "Iteration 19, loss = 0.00367738\n",
      "Iteration 20, loss = 0.00336700\n",
      "Iteration 21, loss = 0.00313311\n",
      "Iteration 22, loss = 0.00294196\n",
      "Iteration 23, loss = 0.00273190\n",
      "Iteration 24, loss = 0.00257803\n",
      "Iteration 25, loss = 0.00237733\n",
      "Iteration 26, loss = 0.00224891\n",
      "Iteration 27, loss = 0.00212703\n",
      "Iteration 28, loss = 0.00207292\n",
      "Iteration 29, loss = 0.00195661\n",
      "Iteration 30, loss = 0.00184960\n",
      "Iteration 31, loss = 0.00184341\n",
      "Iteration 32, loss = 0.00177227\n",
      "Iteration 33, loss = 0.00162370\n",
      "Iteration 34, loss = 0.00155413\n",
      "Iteration 35, loss = 0.00147026\n",
      "Iteration 36, loss = 0.00149280\n",
      "Iteration 37, loss = 0.00134588\n",
      "Iteration 38, loss = 0.00128735\n",
      "Iteration 39, loss = 0.00123937\n",
      "Iteration 40, loss = 0.00119135\n",
      "Iteration 41, loss = 0.00117224\n",
      "Iteration 42, loss = 0.00110286\n",
      "Iteration 43, loss = 0.00106659\n",
      "Iteration 44, loss = 0.00105493\n",
      "Iteration 45, loss = 0.00101297\n",
      "Iteration 46, loss = 0.00098016\n",
      "Iteration 47, loss = 0.00094764\n",
      "Iteration 48, loss = 0.00090884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "acuracy= 0.9928\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying MultiLayer Perceptron\\n\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "import sklearn.metrics as metric\n",
    "import numpy as np\n",
    "\n",
    "print(\"1 hidden Layer, Size =100\\n\")\n",
    "mlp16 = MLPClassifier(solver='adam', hidden_layer_sizes=(100,), activation='tanh',verbose='False') # set the method\n",
    "mlp16.fit(trainX, trainY)                    # training\n",
    "predY_mlp16=mlp16.predict(testX)                      # prediction\n",
    "#print(y_pred)                                      # show the output\n",
    "acc_mlp16=sklearn.metrics.accuracy_score(testY, predY_mlp16)\n",
    "print('\\nacuracy=',acc_mlp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained MLP100 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained MLP100 Model\")\n",
    "file_name = '10_mlp100.sav'\n",
    "pickle.dump(mlp16,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 200 RF estimators\n",
      "Accuracy =  0.996\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying 200 RF estimators\")\n",
    "clf18 = RandomForestClassifier(n_estimators=200,random_state=0,n_jobs=10)\n",
    "clf18.fit(trainX, trainY)\n",
    "predY18 = clf18.predict(testX)\n",
    "acc18 = sklearn.metrics.accuracy_score(testY, predY18)\n",
    "print(\"Accuracy = \",acc18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF200 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF200 Model\")\n",
    "file_name = '11_rf200.sav'\n",
    "pickle.dump(clf18,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 250 RF estimators\n",
      "Accuracy =  0.9976\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying 250 RF estimators\")\n",
    "clf19 = RandomForestClassifier(n_estimators=250,random_state=0,n_jobs=10)\n",
    "clf19.fit(trainX, trainY)\n",
    "predY19 = clf19.predict(testX)\n",
    "acc19 = sklearn.metrics.accuracy_score(testY, predY19)\n",
    "print(\"Accuracy = \",acc19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Trained RF250 Model\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Trained RF250 Model\")\n",
    "file_name = '12_rf250.sav'\n",
    "pickle.dump(clf19,open(file_name,'wb'))\n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving the final Prediction\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predY1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-dec44069a438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     pred_all = np.array([int(predY1[i]), int(predY2[i]), int(predY3[i]), int(predY4[i]), int(predY5[i]), \n\u001b[0m\u001b[1;32m      6\u001b[0m                          \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                          \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp12\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp13\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredY_mlp14\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predY1' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Giving the final Prediction\\n\")\n",
    "\n",
    "predF = []\n",
    "for i in range(1250):\n",
    "    pred_all = np.array([int(predY1[i]), int(predY2[i]), int(predY3[i]), int(predY4[i]), int(predY5[i]), \n",
    "                         int(predY6[i]), int(predY7[i]), int(predY_mlp9[i]), int(predY_mlp10[i]), \n",
    "                         int(predY_mlp11[i]), int(predY_mlp12[i]), int(predY_mlp13[i]), int(predY_mlp14[i]), \n",
    "                         int(predY_mlp15[i]), int(predY_mlp16[i]), int(predY_mlp17[i]), int(predY_mlp18[i])])\n",
    "    w_arr = [acc1,acc2,acc3,acc4,acc5,acc6,acc7,\n",
    "             acc_mlp9,acc_mlp10,acc_mlp11,acc_mlp12,acc_mlp13,\n",
    "             acc_mlp14,acc_mlp15,acc_mlp16,acc_mlp17,acc_mlp18]\n",
    "    predF.append(np.bincount(pred_all, weights = w_arr).argmax())\n",
    "    \n",
    "predF = np.array(predF)\n",
    "accF = sklearn.metrics.accuracy_score(testY, predF)\n",
    "print(\"Final Accuracy = \",accF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
